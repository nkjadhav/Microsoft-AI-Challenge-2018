{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Vectors are created\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Convolution operation requires that kernel dim 4 <= input dim 1.\n\n[CALL STACK]\n    > CNTK::TrainingParameterSchedule<double>::  GetMinibatchSize\n    - CNTK::TrainingParameterSchedule<double>::  GetMinibatchSize\n    - CNTK::NDMask::  MaskedCount (x2)\n    - CNTK::Internal::  Unsqueeze (x4)\n    - CNTK::Function::  ReplacePlaceholders\n    - CNTK::  ImageScaler\n    - CNTK::Function::  ~Function\n    - CNTK_ReleaseModel\n    - RtlRunOnceExecuteOnce\n    - InitOnceExecuteOnce\n    - _crtInitOnceExecuteOnce\n    - CNTK::Function::  InitOutputs\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ddc587bbee75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[0mLoadValidationSet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidationSetFileName\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m#Load Validation Query, Passage Vectors from Validation CTF File\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainAndValidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainSetFileName\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Training and validation methods\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m     \u001b[0mGetPredictionOnEvalSet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtestSetFileName\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msubmissionFileName\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Get Predictions on Evaluation Set\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-ddc587bbee75>\u001b[0m in \u001b[0;36mTrainAndValidate\u001b[1;34m(trainfile)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;31m# ********* Model configuration *******\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m     \u001b[0mmodel_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_input_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassage_input_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[0mpe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-ddc587bbee75>\u001b[0m in \u001b[0;36mcnn_network\u001b[1;34m(queryfeatures, passagefeatures, num_classes)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mconvA1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConvolution2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'convA1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueryfeatures\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#input : 12*50 #output : 4*10*41\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mpoolA1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaxPooling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'poolA1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvA1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#output : 4*5*13\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mconvA2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConvolution2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'convA2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoolA1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#output : 2*4*10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[0mpoolA2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaxPooling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'poolA2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvA2\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#output : 2*2*5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mdenseA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'denseA'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoolA2\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# output : 4\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\rstudio\\lib\\site-packages\\cntk\\ops\\functions.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_symbolic\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[0mFunction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_replace_args_type_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCloneMethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m         \u001b[1;31m# numeric: evaluate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\rstudio\\lib\\site-packages\\cntk\\internal\\swig_helper.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[0mmap_if_possible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\rstudio\\lib\\site-packages\\cntk\\ops\\functions.py\u001b[0m in \u001b[0;36mclone\u001b[1;34m(self, method, substitutions)\u001b[0m\n\u001b[0;32m    662\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnew_node\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mprev_node\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot replace node: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev_node\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" with node: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_node\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\". Neither node can be None.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubstitutions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\rstudio\\lib\\site-packages\\cntk\\cntk_py.py\u001b[0m in \u001b[0;36mclone\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2008\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2009\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_cntk_py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFunction_clone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2011\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclone_flattened\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Convolution operation requires that kernel dim 4 <= input dim 1.\n\n[CALL STACK]\n    > CNTK::TrainingParameterSchedule<double>::  GetMinibatchSize\n    - CNTK::TrainingParameterSchedule<double>::  GetMinibatchSize\n    - CNTK::NDMask::  MaskedCount (x2)\n    - CNTK::Internal::  Unsqueeze (x4)\n    - CNTK::Function::  ReplacePlaceholders\n    - CNTK::  ImageScaler\n    - CNTK::Function::  ~Function\n    - CNTK_ReleaseModel\n    - RtlRunOnceExecuteOnce\n    - InitOnceExecuteOnce\n    - _crtInitOnceExecuteOnce\n    - CNTK::Function::  InitOutputs\n\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import cntk as C\n",
    "from cntk.io import MinibatchSource, CTFDeserializer, StreamDef, StreamDefs, INFINITELY_REPEAT, FULL_DATA_SWEEP\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning) \n",
    "\n",
    "\n",
    "\n",
    "#Initialize Global variables\n",
    "validation_query_vectors = []\n",
    "validation_passage_vectors = []\n",
    "validation_labels = []   \n",
    "q_max_words=12\n",
    "p_max_words=50\n",
    "emb_dim=50\n",
    "\n",
    "## The following LoadValidationSet method reads ctf format validation file and creates query, passage feature vectors and also copies labels for each pair.\n",
    "## the created vectors will be useful to find metrics on validation set after training each epoch which will be useful to decide the best model \n",
    "def LoadValidationSet(validationfile):\n",
    "    f = open(validationfile,'r',encoding=\"utf-8\")\n",
    "    for line in f:\n",
    "        tokens = line.strip().split(\"|\")  \n",
    "        #tokens[0] will be empty token since the line is starting with |\n",
    "        x1 = tokens[1].replace(\"qfeatures\",\"\").strip() #Query Features\n",
    "        x2 = tokens[2].replace(\"pfeatures\",\"\").strip() # Passage Features\n",
    "        y = tokens[3].replace(\"labels\",\"\").strip() # labels\n",
    "        x1 = [float(v) for v in x1.split()]\n",
    "        x2 = [float(v) for v in x2.split()]\n",
    "        y = [int(w) for w in y.split()]        \n",
    "        y = y[1] # label will be at index 1, i.e. if y = \"1 0\" then label=0 else if y=\"0 1\" then label=1\n",
    "\n",
    "        validation_query_vectors.append(x1)\n",
    "        validation_passage_vectors.append(x2)\n",
    "        validation_labels.append(y)\n",
    "\n",
    "        #print(\"1\")\n",
    "    \n",
    "    print(\"Validation Vectors are created\")\n",
    "\n",
    "#The following method defines a CNN network which has series of convolution and max pooling steps on query features and passage features and then a merge step and it follows a fully connected layer\n",
    "def cnn_network(queryfeatures, passagefeatures, num_classes):\n",
    "    with C.layers.default_options(activation=C.ops.relu, pad=False):\n",
    "#         convA1 = C.layers.Convolution2D((3,10),4,pad=True,activation=C.tanh,name='convA1')(queryfeatures) #input : 12*50 #output : 4*10*41\n",
    "#         poolA1 = C.layers.MaxPooling((2,3),(2,3),name='poolA1')(convA1)  #output : 4*5*13\n",
    "#         convA2 = C.layers.Convolution2D((2,4),2,pad=True,activation=C.tanh,name='convA2')(poolA1) #output : 2*4*10\n",
    "#         poolA2 = C.layers.MaxPooling((2,2),(2,2),name='poolA2')(convA2)  #output : 2*2*5\n",
    "#         denseA = C.layers.Dense(num_classes*num_classes,activation=C.tanh,name='denseA')(poolA2)  # output : 4\n",
    "         \n",
    "#         convB1 = C.layers.Convolution2D((5,10),4,pad=True,activation=C.tanh,name='convB1')(passagefeatures) #input : 50*50  #output : 4*46*41\n",
    "#         poolB1 = C.layers.MaxPooling((5,5),(5,5),name='poolB1')(convB1) #output : 4*9*8\n",
    "#         convB2 = C.layers.Convolution2D((3,3),2,pad=True,activation=C.tanh,name='convB2')(poolB1) #output : 2*7*6\n",
    "#         poolB2 = C.layers.MaxPooling((2,2),(2,2),name='poolB2')(convB2) #output : 2*3*3\n",
    "#         denseB = C.layers.Dense(num_classes*num_classes,activation=C.tanh,name='denseB')(poolB2)  # output : 4\n",
    "\n",
    "        convA1 = C.layers.Convolution2D((6,20),8,pad=False,activation=C.tanh,name='convA1')(queryfeatures) #input : 12*50 #output : 4*10*41\n",
    "        poolA1 = C.layers.MaxPooling((4,6),(4,6),name='poolA1')(convA1)  #output : 4*5*13\n",
    "        convA2 = C.layers.Convolution2D((4,4),4,pad=False,activation=C.tanh,name='convA2')(poolA1) #output : 2*4*10\n",
    "        poolA2 = C.layers.MaxPooling((4,4),(4,4),name='poolA2')(convA2)  #output : 2*2*5\n",
    "        denseA = C.layers.Dense(num_classes*num_classes,activation=C.tanh,name='denseA')(poolA2)  # output : 4\n",
    "         \n",
    "        convB1 = C.layers.Convolution2D((10,20),8,pad=False,activation=C.tanh,name='convB1')(passagefeatures) #input : 50*50  #output : 4*46*41\n",
    "        poolB1 = C.layers.MaxPooling((10,10),(10,10),name='poolB1')(convB1) #output : 4*9*8\n",
    "        convB2 = C.layers.Convolution2D((6,6),4,pad=False,activation=C.tanh,name='convB2')(poolB1) #output : 2*7*6\n",
    "        poolB2 = C.layers.MaxPooling((4,4),(4,4),name='poolB2')(convB2) #output : 2*3*3\n",
    "        denseB = C.layers.Dense(num_classes*num_classes,activation=C.tanh,name='denseB')(poolB2)  # output : 4\n",
    "\n",
    "        mergeQP     = C.element_times(denseA,denseB) # output : 4\n",
    "\n",
    "        model   = C.layers.Dense(num_classes, activation= C.softmax ,name=\"overall\")(mergeQP) #outupt : 2 \n",
    "        \n",
    "\n",
    "    return model\n",
    "\n",
    "def create_reader(path, is_training, query_total_dim, passage_total_dim, label_total_dim):\n",
    "    return MinibatchSource(CTFDeserializer(path, StreamDefs( queryfeatures = StreamDef(field='qfeatures', shape=query_total_dim,is_sparse=False), \n",
    "                                                            passagefeatures = StreamDef(field='pfeatures', shape=passage_total_dim,is_sparse=False), \n",
    "                                                            labels   = StreamDef(field='labels', shape=label_total_dim,is_sparse=False)\n",
    "                                                            )), \n",
    "                           randomize=is_training, max_sweeps = INFINITELY_REPEAT if is_training else FULL_DATA_SWEEP)\n",
    "\n",
    "def TrainAndValidate(trainfile):\n",
    "\n",
    "    #*****Hyper-Parameters******\n",
    "    q_max_words= 12\n",
    "    p_max_words = 50\n",
    "    emb_dim = 50\n",
    "    num_classes = 2\n",
    "    minibatch_size = 1024\n",
    "    epoch_size = 65536 #No.of samples in training set #194278 #100000 #777117\n",
    "    total_epochs = 10 #200 #Total number of epochs to run \n",
    "    query_total_dim = q_max_words*emb_dim\n",
    "    label_total_dim = num_classes\n",
    "    passage_total_dim = p_max_words*emb_dim\n",
    "\n",
    "\n",
    "    #****** Create placeholders for reading Training Data  ***********\n",
    "    query_input_var =  C.ops.input_variable((1,q_max_words,emb_dim),np.float32,is_sparse=False)\n",
    "    passage_input_var =  C.ops.input_variable((1,p_max_words,emb_dim),np.float32,is_sparse=False)\n",
    "    output_var = C.input_variable(num_classes,np.float32,is_sparse = False)\n",
    "    train_reader = create_reader(trainfile, True, query_total_dim, passage_total_dim, label_total_dim)\n",
    "    input_map = { query_input_var : train_reader.streams.queryfeatures, passage_input_var:train_reader.streams.passagefeatures, output_var : train_reader.streams.labels}\n",
    "\n",
    "    # ********* Model configuration *******\n",
    "    model_output = cnn_network(query_input_var, passage_input_var, num_classes)\n",
    "    loss = C.binary_cross_entropy(model_output, output_var)\n",
    "    pe = C.classification_error(model_output, output_var)\n",
    "    lr_per_minibatch = C.learning_rate_schedule(0.5, C.UnitType.minibatch)   # Learning Rate\n",
    "    learner = C.adagrad(model_output.parameters, lr=lr_per_minibatch) #adagrad\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=total_epochs)\n",
    "\n",
    "    #************Create Trainer with model_output object, learner and loss parameters*************  \n",
    "    trainer = C.Trainer(model_output, (loss, pe), learner, progress_printer)\n",
    "    C.logging.log_number_of_parameters(model_output) ; print()\n",
    "\n",
    "    # **** Train the model in batchwise mode *****\n",
    "    for epoch in range(total_epochs):       # loop over epochs\n",
    "        print(\"Epoch : \",epoch)\n",
    "        sample_count = 0\n",
    "        while sample_count < epoch_size:  # loop over minibatches in the epoch\n",
    "            data = train_reader.next_minibatch(min(minibatch_size, epoch_size - sample_count), input_map=input_map) # fetch minibatch.\n",
    "            trainer.train_minibatch(data)        # training step\n",
    "            sample_count += data[output_var].num_samples   # count samples processed so far\n",
    "\n",
    "        trainer.summarize_training_progress()\n",
    "                \n",
    "        model_output.save(\"CNN_{}.dnn\".format(epoch)) # Save the model for every epoch\n",
    "    \n",
    "#         #*** Find metrics on validation set after every epoch ******#  (Note : you can skip doing this for every epoch instead to optimize the time, do it after every k epochs)\n",
    "#         predicted_labels=[]\n",
    "#         for i in range(len(validation_query_vectors)):\n",
    "#             queryVec   = np.array(validation_query_vectors[i],dtype=\"float32\").reshape(1,q_max_words,emb_dim)\n",
    "#             passageVec = np.array(validation_passage_vectors[i],dtype=\"float32\").reshape(1,p_max_words,emb_dim)\n",
    "#             scores = model_output(queryVec,passageVec)[0]   # do forward-prop on model to get score  \n",
    "#             predictLabel = 1 if scores[1]>=scores[0] else 0\n",
    "#             predicted_labels.append(predictLabel) \n",
    "#         metrics = precision_recall_fscore_support(np.array(validation_labels), np.array(predicted_labels), average='binary')\n",
    "#         #print(\"precision : \"+str(metrics[0])+\" recall : \"+str(metrics[1])+\" f1 : \"+str(metrics[2])+\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    return model_output\n",
    "\n",
    "## The following GetPredictionOnEvalSet method reads all query passage pair vectors from CTF file and does forward prop with trained model to get similarity score\n",
    "## after getting scores for all the pairs, the output will be written into submission file. \n",
    "def GetPredictionOnEvalSet(model,testfile,submissionfile):\n",
    "    global q_max_words,p_max_words,emb_dim\n",
    "\n",
    "    f = open(testfile,'r',encoding=\"utf-8\")\n",
    "    all_scores={} # Dictionary with key = query_id and value = array of scores for respective passages\n",
    "    for line in f:\n",
    "        tokens = line.strip().split(\"|\")  \n",
    "        #tokens[0] will be empty token since the line is starting with |\n",
    "        x1 = tokens[1].replace(\"qfeatures\",\"\").strip() #Query Features\n",
    "        x2 = tokens[2].replace(\"pfeatures\",\"\").strip() # Passage Features\n",
    "        query_id = tokens[3].replace(\"qid\",\"\").strip() # Query_id\n",
    "        x1 = [float(v) for v in x1.split()]\n",
    "        x2 = [float(v) for v in x2.split()]    \n",
    "        queryVec   = np.array(x1,dtype=\"float32\").reshape(1,q_max_words,emb_dim)\n",
    "        passageVec = np.array(x2,dtype=\"float32\").reshape(1,p_max_words,emb_dim)\n",
    "        score = model(queryVec,passageVec)[0][1] # do forward-prop on model to get score\n",
    "        if(query_id in all_scores):\n",
    "            all_scores[query_id].append(score)\n",
    "        else:\n",
    "            all_scores[query_id] = [score]\n",
    "    fw = open(submissionfile,\"w\",encoding=\"utf-8\")\n",
    "    for query_id in all_scores:\n",
    "        scores = all_scores[query_id]\n",
    "        scores_str = [str(sc) for sc in scores] # convert all scores to string values\n",
    "        scores_str = \"\\t\".join(scores_str) # join all scores in list to make it one string with  tab delimiter.  \n",
    "        fw.write(query_id+\"\\t\"+scores_str+\"\\n\")\n",
    "    fw.close()\n",
    "    print(\"Submission file created. \")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    trainSetFileName = \"TrainData.ctf\"\n",
    "    validationSetFileName = \"ValidationData.ctf\"\n",
    "    testSetFileName = \"EvaluationData.ctf\"\n",
    "    submissionFileName = \"answer.tsv\"\n",
    "   \n",
    "    LoadValidationSet(validationSetFileName)    #Load Validation Query, Passage Vectors from Validation CTF File\n",
    "    model = TrainAndValidate(trainSetFileName) # Training and validation methods    \n",
    "    GetPredictionOnEvalSet(model,testSetFileName,submissionFileName) # Get Predictions on Evaluation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
